{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "In class, we have learned logistic regression which is widely used on classification tasks. We have also learned gradient descent, which is a first-order iterative optimization algorithm for finding the minimum of a function. Now we will make some experiments to show how gradient descent can be used on logistic regression. We are going to use `numpy` and `matplotlib` packages in our experiments, the first thing we need to do is to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# time packages are used to measure the convergence speed\n",
    "import time\n",
    "# there might be some warnings due to the different versions of python and packages you installed.\n",
    "# here we choose to suppress these warnings.\n",
    "# but don't ignore warnings unless you know you are absolutely right!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our experiment consists of two sub-experiments:\n",
    "- Logistic regression with batch gradient descent.\n",
    "- Logistic regression with stochastic gradient descent.\n",
    "\n",
    "Each sub-experiment consists of 3 steps:\n",
    "1. Generation of training dataset.\n",
    "2. Implementation of algorithms.\n",
    "3. Visualization of experimental results.\n",
    "\n",
    "And we will have an analytical question in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with batch gradient descent\n",
    "#### Generation of training dataset\n",
    "Given a feature vector $\\boldsymbol x$, the linear combination of features can be written as:\n",
    "$$\n",
    "\\sum_{i=1}^n {\\boldsymbol w_i \\boldsymbol x_i} + b\n",
    "$$\n",
    "where $\\boldsymbol w$ is the weight vector and $b$ is bias. We can easily compact bias into matrix multiplication by adding another dimension:\n",
    "$$\n",
    "\\sum_{i=1}^n {\\boldsymbol w_i \\boldsymbol x_i} + b = \\boldsymbol{\\theta}^{T} \\boldsymbol{x} \n",
    "$$\n",
    "where $\\boldsymbol{x} = [\\boldsymbol x_1\\cdots \\boldsymbol x_n, 1]^T, \\boldsymbol\\theta = [\\boldsymbol w_1\\cdots \\boldsymbol w_n, b]^T$. Logistic model is defined by by applying the *sigmoid* function on the linear combination of features \n",
    "$$h_{\\boldsymbol \\theta}(\\boldsymbol{x})= g(\\boldsymbol{\\theta}^{T} \\boldsymbol{x}) = \\frac{1}{1+\\exp \\left(-\\boldsymbol{\\theta}^{T} \\boldsymbol{x}\\right)}.$$\n",
    "Because sigmoid function is bounded in $(0, 1)$, we can consider $h_{\\boldsymbol \\theta}$ as probability. Now define our sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate training data based on the defination of logistic model. Here we only consider the simplest case, which means the number of features is set as $1$. You can choose to add bias on the model. If you set `with_bias` as true, the last element of $\\boldsymbol \\theta$ is the bias. We also just set $\\boldsymbol \\theta = \\boldsymbol 1$, where $\\boldsymbol 1$ is a vector with all components equal to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1\n",
    "with_bias = True\n",
    "theta_ = np.ones(num_features + int(with_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume we have an observation matrix $\\boldsymbol X$, we can compute the probability vector $\\boldsymbol p$ of these samples through $h_{\\boldsymbol \\theta}(\\boldsymbol{x}).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(X, theta):\n",
    "    # X@y is equivalent to np.matmul(X, y)\n",
    "    return sigmoid(X@theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label vector $\\boldsymbol y$ will be determined through a threshold based on the probability vector\n",
    "$$\n",
    "\\boldsymbol{y}_i = \\left\\{\n",
    "\\begin{matrix}\n",
    "1 & {\\text{if } \\boldsymbol{p}_i > 0.5}\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{matrix}\\right..\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(X, theta):\n",
    "    return (predict_probability(X, theta) > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate the above content, we can write a function to randomly generate training data of binary classification. You just need to input the number of samples, weight vector and bias option and the function will return observation matrix $\\boldsymbol X$ and label vector $\\boldsymbol y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_classification_data(num_samples, theta, bias = True):\n",
    "    if bias == False:\n",
    "        X = (np.random.rand(num_samples, theta.shape[0]) - 0.5) * 4\n",
    "    else:\n",
    "        # place the center of X on (-bias)\n",
    "        X = (np.random.rand(num_samples, theta.shape[0] - 1) - 0.5) * 4 - theta[-1]\n",
    "        # add another dimension for bias\n",
    "        X = np.insert(X, X.shape[1], 1, axis=1)\n",
    "    y = predict_label(X, theta)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate $100$ training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_binary_classification_data(100, theta_, with_bias)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of algorithms\n",
    "We know that using maximal likelihood to estimate the parameters of logistic regression model is equivalent to maximize:\n",
    "$$\n",
    "\\mathcal l(\\boldsymbol \\theta)=\\sum_{i=1}^{m} y^{(i)} \\log h_{\\boldsymbol \\theta}\\left(\\boldsymbol{x}^{(i)}\\right)+\\left(1- y^{(i)}\\right) \\log \\left(1-h_{\\boldsymbol \\theta}\\left(\\boldsymbol{x}^{(i)}\\right)\\right).\n",
    "$$\n",
    "And based on above formula, we can define our log-loss function in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(X, y, theta):\n",
    "    y_expand = np.vstack([1 - y, y])\n",
    "    predicted_prob = predict_probability(X, theta)\n",
    "    predicted_prob = np.vstack([1-predicted_prob, predicted_prob])\n",
    "    # here we add an small vaue to avoid Nan\n",
    "    predicted_prob += 1e-4 * (predicted_prob == 0).astype(float)\n",
    "    # X*y is element-wise production\n",
    "    return -np.sum(y_expand * np.log(predicted_prob))    \n",
    "print(log_loss(X, y, theta_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also I want to mention that, for classification problems, prediction accuracy is a more intuitive way to measure the performance of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, theta):\n",
    "    predicted_label = predict_label(X, theta)\n",
    "    return np.sum((predicted_label == y).astype(float)) / y.shape[0]\n",
    "print(accuracy(X, y, theta_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $l(\\boldsymbol\\theta)$ can be computed through\n",
    "$$\n",
    "\\frac{\\partial l(\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta_{j}}=\\sum_{i=1}^{m}\\left(y^{(i)}-h_{\\boldsymbol\\theta}\\left(\\boldsymbol x^{(i)}\\right)\\right) \\boldsymbol x_{j}^{(i)}.\n",
    "$$\n",
    "In gradient descent, we update the weights through\n",
    "$$\n",
    "\\boldsymbol\\theta := \\boldsymbol\\theta - \\alpha \\nabla J(\\boldsymbol\\theta)\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Here we want to maximize $\\mathcal l(\\boldsymbol \\theta)$, the update rule becomes\n",
    "$$\n",
    "\\boldsymbol \\theta_{j}:=\\boldsymbol \\theta_{j}+\\alpha \\sum_{i=1}^{m}\\left(y^{(i)}-h_{\\boldsymbol \\theta}\\left(\\boldsymbol x^{(i)}\\right)\\right) \\boldsymbol x_{j}^{(i)} \\text { for every } j.\n",
    "$$\n",
    "Now let us transfer this formula into codes. Instead of using a foor-loop, we prefer to use matrix operations because it's much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_step_bgd(X, y, theta, alpha = 0.1):\n",
    "    theta = theta + alpha * X.transpose() @ (y - sigmoid(X @ theta))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is known as batch gradient descent (BGD) because we only update the weights after we have seen all training samples. \n",
    "#### Visualization of experimental results\n",
    "So far we have finished BGD algorithm. We can write some simple codes to test our implementation immediately. In the following code, we will train our logistic model for a certain amount of epochs. During the training process, we will print the log-loss and accuracy. We also plot our model step by step to better illustrate how the weights change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero initialization\n",
    "theta = np.zeros_like(theta_)\n",
    "alpha = 0.002\n",
    "plt.scatter(X[:,0], y)\n",
    "# plot the probability curve\n",
    "x_uni = np.array([np.linspace(np.min(X[:, 0]), np.max(X[:,0]), 100)]).transpose()\n",
    "if with_bias:\n",
    "    x_uni = np.insert(x_uni, 1, 1, axis=1)\n",
    "y_ = predict_probability(x_uni, theta)\n",
    "plt.plot(x_uni[:,0], y_, label=\"initial model\")\n",
    "print(r'iter {}, log-loss: {}, accuracy: {}'.format(0, log_loss(X, y, theta), accuracy(X, y, theta)))\n",
    "plot_id = 25\n",
    "# 1600 iteration steps\n",
    "for iter_step in range(1600):\n",
    "    theta = iteration_step_bgd(X, y, theta, alpha)\n",
    "    if iter_step+1 == plot_id:\n",
    "        plot_id *= 2\n",
    "        print(r'iter {}, log-loss: {}, accuracy: {}'.format(iter_step+1, log_loss(X, y, theta), accuracy(X, y, theta)))\n",
    "        y_ = predict_probability(x_uni, theta)\n",
    "        plt.plot(x_uni[:,0], y_, label=r\"iter {}\".format(iter_step+1))\n",
    "    iter_step += 1\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y (probability)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with stochastic gradient descent\n",
    "#### Generation of training dataset\n",
    "In class we also learned stochastic gradient descent (SGD), in which we only use one sample to compute the gradient and update the weight:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{for } i=&1\\cdots m,\\\\\n",
    "&\\boldsymbol \\theta_{j}:=\\boldsymbol \\theta_{j}+\\alpha\\left(y^{(i)}-h_{\\boldsymbol \\theta}\\left(\\boldsymbol x^{(i)}\\right)\\right) \\boldsymbol x_{j}^{(i)} \\text { for every } j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "We know that SGD has a much faster convergence speed than BGD especially on large dataset. Now let's do some experiments to verify it. First, we need to generate a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features = 2\n",
    "theta_ = np.ones(number_features + int(with_bias))\n",
    "X, y = generate_binary_classification_data(50000, theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Implementation of algorithms\n",
    " To compare the speed, we need to train two logistic models using BGD and SGD respectively. Now initailize two weight vectors with zero vector, the first one will be updated through BGD and the second one will be updated through SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight of our first model using bgd\n",
    "theta_bgd = np.zeros_like(theta_)\n",
    "# weight of our second model using sgd\n",
    "theta_sgd = np.zeros_like(theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Now please implement SGD by completing following code. Note that in each iteration step of SGD, we will only see one training sample. Therefore, `X` is a feature vector rather than a matrix, and `y` is a scalar rather than a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_step_sgd(X, y, theta, alpha = 0.1):\n",
    "    # write your implementation here.\n",
    "    theta = theta + alpha * (y - sigmoid(X @ theta)) * X\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of experimental results\n",
    "Now we have implemented SGD algorithm. We still need to write some scripts to test our models. We have a `train_for_seconds` function here. You just need to input observation matrix $\\boldsymbol X$ and label vector $\\boldsymbol y$, weight vector $\\boldsymbol\\theta$ and flag of gradient descent method you want to use (0 for BGD and 1 for SGD). It will help you to train the model based on you implementation. Specifically, it will train the input model `t` seconds, and return elapsed time and corresponding log-losses which can be directly used to draw the learning curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_seconds(X, y, theta, gd_method = 0, alpha = 1, t = 2, k = 5, n = 500):\n",
    "    iter_step_id = 0\n",
    "    time_axis = [0]\n",
    "    train_loss = [log_loss(X, y, theta)]\n",
    "    stored_t = []\n",
    "    stored_weights = []\n",
    "    if gd_method == 0:\n",
    "        alpha /= X.shape[0]\n",
    "    start = time.perf_counter()\n",
    "    while True:\n",
    "        # train the model\n",
    "        if gd_method == 0:\n",
    "            theta = iteration_step_bgd(X, y, theta, alpha)\n",
    "        elif gd_method == 1:\n",
    "            theta = iteration_step_sgd(X[iter_step_id%X.shape[0]], y[iter_step_id%y.shape[0]], theta, alpha)\n",
    "        curr = time.perf_counter()\n",
    "        iter_step_id += 1\n",
    "        if iter_step_id % k == 0:\n",
    "            stored_t.append(curr - start)\n",
    "            stored_weights.append(theta.copy())\n",
    "        if curr - start > t:\n",
    "            break\n",
    "    n = min(n, len(stored_t))\n",
    "    uni_samples = [ int(x) for x in np.floor(np.linspace(0, len(stored_t)-1, n))]\n",
    "    for us in uni_samples:\n",
    "        time_axis.append(stored_t[us])\n",
    "        train_loss.append(log_loss(X, y, stored_weights[us]))\n",
    "    time_axis.append(curr - start)\n",
    "    train_loss.append(log_loss(X, y, theta))\n",
    "    return time_axis, train_loss, iter_step_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready. We will spend the same time training these two models. After that, the learning curves of both models will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 2 seconds\n",
    "t_bgd, loss_bgd, num_iter_bgd = train_for_seconds(X, y, theta_bgd, gd_method = 0, k = 1)\n",
    "t_sgd, loss_sgd, num_iter_sgd = train_for_seconds(X, y, theta_sgd, gd_method = 1, k = 5)\n",
    "print(\"number of iteration (bgd):\", num_iter_bgd)\n",
    "print(\"number of iteration (sgd):\", num_iter_sgd)\n",
    "# plot the learning curve \n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(t_bgd, loss_bgd)\n",
    "plt.plot(t_sgd, loss_sgd)\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"log-loss\")\n",
    "plt.legend([\"BGD\", \"SGD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical question \n",
    "#### Q2. If your implementation is correct, you should see that the log-loss decreases first and then oscillates near the minimum. Change the number of training samples to get different results, then compare the curves and explain what you have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
