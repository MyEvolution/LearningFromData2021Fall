{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Regularization\n",
    "In class, we have learned linear regression. In this section, we will take through a simple experiment to see how it works. We are going to use `numpy` and `matplotlib` packages in Python, the first thing we need to do is to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# for 3D plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "# there might be some warnings due to the different versions of python and packages you installed.\n",
    "# here we choose to suppress these warnings.\n",
    "# but don't ignore warnings unless you know you are absolutely right!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our experiment consists of three steps:\n",
    "1. Generation of training dataset.\n",
    "2. Implementation of our models\n",
    "3. Visualization of experimental results.\n",
    "\n",
    "And we will have an analytical question in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of training dataset\n",
    "Linear model is defined as\n",
    "$$\n",
    "\\boldsymbol{y}=\\boldsymbol {X \\boldsymbol\\theta}+ \\boldsymbol \\varepsilon,\n",
    "$$\n",
    "    where $\\boldsymbol X = [\\boldsymbol x^{(1)}\\cdots \\boldsymbol x^{(m)}]^T$ is the observation matrix which consists of $m$ samples, $\\boldsymbol x^{(i)}$ is the feature vector of $i$-th sample, whose length is also known as *the number of features*; $\\boldsymbol \\theta$ is the weight vector and $\\boldsymbol\\varepsilon$ is the Gaussian noise. We will generate training data based on the defination of linear model. Here the number of features is set as $2$. For simplicity, we just set $\\boldsymbol \\theta = \\boldsymbol 1$, where $\\boldsymbol 1$ is a vector with all components equal to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 2\n",
    "theta_ = np.ones(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the the number of samples, we generate a random observation matrix $\\boldsymbol X$. The corresponding labels $\\boldsymbol y$ is computed through $\\boldsymbol{X\\theta}$ plus some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "# range of x: [-0.5, 0.5]\n",
    "X = np.random.rand(num_samples, num_features)-0.5\n",
    "# X@y is equivalent to np.matmul(X, y)\n",
    "y = X@theta_ + np.random.normal(scale=0.2, size=(num_samples))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of our models\n",
    "Note that in real life we can only get $\\boldsymbol {X}$ and $\\boldsymbol {y}$. $\\boldsymbol\\theta$ is the weight vector we want to esimate through linear regression. To minimize the noise effect, we do a minimization problem\n",
    "$$\n",
    "\\min _{\\boldsymbol\\theta}\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2}\n",
    "$$\n",
    "and we can get the best estimator by solving the normal equation\n",
    "$$\n",
    "\\boldsymbol X^{T} \\boldsymbol {X\\theta}=\\boldsymbol{X}^T \\boldsymbol{y},\\\\\n",
    "\\boldsymbol \\theta =  (\\boldsymbol X^{T} \\boldsymbol X)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}.\n",
    "$$\n",
    "Transfer the above formula into code and we get the analytical solution of linear regression. In review section we learned that the inversion of matrix can be computed through `numpy.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    return np.linalg.inv((X.transpose()@X))@X.transpose()@y\n",
    "theta_linear_regression = linear_regression(X, y)\n",
    "print(theta_linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we have implemented our linear regression model. We can also add a regularization in linear model and we have\n",
    "$$\n",
    "\\min _{\\boldsymbol\\theta}\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2}+\\alpha\\|\\boldsymbol{\\theta}\\|^{2},\n",
    "$$\n",
    "where $\\alpha \\geq 0$ is a hyper-parameter and defines the strength of regularization. This method is known as *ridge regression*. Minimizing the loss $\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2}+\\alpha\\|\\boldsymbol{\\theta}\\|^{2}$ is equivalent to solve the normal equation \n",
    "$$\n",
    "\\left(\\boldsymbol X^{T} \\boldsymbol X+\\alpha \\boldsymbol I\\right) \\boldsymbol{\\theta}=\\boldsymbol X^{T} \\boldsymbol{y},\\\\\n",
    "\\boldsymbol{\\theta} = \\left(\\boldsymbol X^{T} \\boldsymbol X+\\alpha \\boldsymbol I\\right)^{-1}\\boldsymbol X^{T} \\boldsymbol{y}\n",
    "$$\n",
    "#### Q1. Now, please implement ridge regression by completing following code. You should give an analytical solution instead of using gradient descent to get a numerical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, alpha = 1):\n",
    "    return np.linalg.inv(X.transpose()@X + alpha * np.eye(X.shape[1]) )@X.transpose()@y\n",
    "theta_ridge_regression = ridge_regression(X, y)\n",
    "print(theta_ridge_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of experimental results\n",
    "So far we have finished the implementation of linear regression and ridge regression models. To better understand our work, we will plot train data, linear regression model and ridge regression model on the same figure using `matplotlib` package. *Note that because the number of features is $2$, we need to use 3D figure for visualization. If you are running jupyter notebook on PC, you should be able to drag the mouse to rotate the figure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "x_axis = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 20)\n",
    "y_axis = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 20)\n",
    "x_axis, y_axis = np.meshgrid(x_axis, y_axis)\n",
    "z_axis_1 = theta_linear_regression[0] * x_axis + theta_linear_regression[1] * y_axis\n",
    "z_axis_2 = theta_ridge_regression[0] * x_axis + theta_ridge_regression[1] * y_axis\n",
    "ax.plot_surface(x_axis, y_axis, z_axis_1, alpha = 0.6)\n",
    "ax.plot_surface(x_axis, y_axis, z_axis_2, alpha = 0.6)\n",
    "fake_sct_1 = ax.scatter([], [], [], label=\"linear regression\")\n",
    "fake_sct_2 = ax.scatter([], [], [], label=\"ridge regression\")\n",
    "sct = ax.scatter(X[:, 0], X[:, 1], y, label=\"train data\", color='red')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical question\n",
    "#### Q2. So far we have implemented linear regression and ridge regression. Now let us try to use our models on some special datasets. Run following code several timesto compare the difference between linear regression and ridge regression. Analyze the role of regularization in ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ill-posed problem\n",
    "X = np.random.rand(1, num_features)\n",
    "for i in range(1, num_samples):\n",
    "    X = np.vstack([X, i * X[0]] )\n",
    "y = X@theta_ + np.random.normal(scale=1, size=(num_samples))\n",
    "theta_linear_regression = linear_regression(X, y)\n",
    "print(theta_linear_regression)\n",
    "theta_ridge_regression = ridge_regression(X, y, 0.1)\n",
    "print(theta_ridge_regression)\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "x_axis = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 20)\n",
    "y_axis = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 20)\n",
    "x_axis, y_axis = np.meshgrid(x_axis, y_axis)\n",
    "z_axis_1 = theta_linear_regression[0] * x_axis + theta_linear_regression[1] * y_axis\n",
    "z_axis_2 = theta_ridge_regression[0] * x_axis + theta_ridge_regression[1] * y_axis\n",
    "ax.plot_surface(x_axis, y_axis, z_axis_1, alpha = 0.6)\n",
    "ax.plot_surface(x_axis, y_axis, z_axis_2, alpha = 0.6)\n",
    "fake_sct_1 = ax.scatter([], [], [], label=\"linear regression\")\n",
    "fake_sct_2 = ax.scatter([], [], [], label=\"ridge regression\")\n",
    "ax.scatter(X[:, 0], X[:, 1], y, label=\"train data\", color='red')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
